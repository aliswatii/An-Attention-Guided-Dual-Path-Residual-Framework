{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd782c7e-6e6d-496a-9161-6f77c1555060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Multiply, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787f3e09-478f-4339-a4f2-9459eff24b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset directory\n",
    "dataset_dir = r\"dir\"  # Replace with your dataset path\n",
    "\n",
    "# Function to load image paths and corresponding labels\n",
    "def load_image_paths_and_labels(dataset_dir):\n",
    "    image_paths = []  # To store all image file paths\n",
    "    labels = []       # To store corresponding labels as integers\n",
    "    class_names = sorted(os.listdir(dataset_dir))  # Sort class names alphabetically for consistent labeling\n",
    "\n",
    "    for label, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(dataset_dir, class_name)\n",
    "        if os.path.isdir(class_dir):  # Ensure it's a directory\n",
    "            for file_name in os.listdir(class_dir):\n",
    "                if file_name.lower().endswith(('.png', '.jpg', '.jpeg')):  # Check for valid image extensions\n",
    "                    file_path = os.path.join(class_dir, file_name)\n",
    "                    image_paths.append(file_path)\n",
    "                    labels.append(label)  # Assign label based on the class index\n",
    "\n",
    "    return image_paths, labels, class_names\n",
    "\n",
    "# Load data\n",
    "image_paths, labels, class_names = load_image_paths_and_labels(dataset_dir)\n",
    "\n",
    "# Convert labels to numpy array\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"Loaded {len(image_paths)} images from {len(class_names)} classes:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"Class {i}: {class_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0219e339-b15f-4f40-938d-6792d7e9718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation settings\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d700afa3-a033-4367-bc1f-aa596eccbef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom generator to yield dual inputs\n",
    "def dual_input_generator(image_paths, labels, batch_size, input_shape):\n",
    "    while True:\n",
    "        indices = np.random.choice(len(image_paths), batch_size)\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "\n",
    "        for i in indices:\n",
    "            img = tf.keras.utils.load_img(image_paths[i], target_size=input_shape[:2])\n",
    "            img = tf.keras.utils.img_to_array(img) / 255.0  # Normalize to [0, 1]\n",
    "            batch_images.append(img)\n",
    "            batch_labels.append(labels[i])\n",
    "\n",
    "        batch_images = np.array(batch_images)\n",
    "        batch_labels = tf.keras.utils.to_categorical(batch_labels, num_classes=4)  # Convert labels to one-hot\n",
    "\n",
    "        # Yield two identical inputs for dual-stream model\n",
    "        yield (batch_images, batch_images), batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc05da9-7630-4a8e-b659-5ed27bb2bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a CNN stream by creating a custom layer with ResNet50 features\n",
    "class CNNStream(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_shape, name_suffix):\n",
    "        super(CNNStream, self).__init__(name=f\"CNNStream_{name_suffix}\")\n",
    "        self.base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape, name=f'resnet_{name_suffix}')\n",
    "        self.pooling = GlobalAveragePooling2D()\n",
    "        self.fc1 = Dense(128, activation='relu')\n",
    "        self.dropout = Dropout(0.4)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.base_model(inputs)\n",
    "        x = self.pooling(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a077d700-cd76-41ed-9613-3dd34518200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the soft attention mechanism\n",
    "def soft_attention(fusion_features):\n",
    "    attention = Dense(128, activation='sigmoid')(fusion_features)\n",
    "    attention = Dense(1, activation='sigmoid')(attention)\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddd62a5-61c8-40a4-a9c9-673bde93da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Dual-Stream Model with Soft Attention\n",
    "def build_dual_stream_model(input_shape, num_classes):\n",
    "    # Stream 1 with unique name suffix\n",
    "    input1 = Input(shape=input_shape)\n",
    "    stream1_features = CNNStream(input_shape, 'stream1')(input1)\n",
    "    \n",
    "    # Stream 2 with a different name suffix\n",
    "    input2 = Input(shape=input_shape)\n",
    "    stream2_features = CNNStream(input_shape, 'stream2')(input2)\n",
    "\n",
    "    # Concatenate the outputs of both streams\n",
    "    fusion_features = Concatenate()([stream1_features, stream2_features])\n",
    "\n",
    "    # Apply the soft attention mechanism\n",
    "    attention = soft_attention(fusion_features)\n",
    "    \n",
    "    # Multiply attention weights with the fusion features\n",
    "    attended_features = Multiply()([fusion_features, attention])\n",
    "\n",
    "    # Fully connected layer for classification\n",
    "    x = Dense(256, activation='relu')(attended_features)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = Model(inputs=[input1, input2], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1361425-0618-47e2-91a0-b3ec2c046df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KFold\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Parameters\n",
    "input_shape = (174, 208, 3)  # Adjust based on your image size\n",
    "num_classes = 4  # 4 classes\n",
    "batch_size = 32\n",
    "epochs = 40\n",
    "\n",
    "# Store results\n",
    "fold_accuracies = []\n",
    "fold_losses = []\n",
    "\n",
    "# for fold, (train_idx, val_idx) in enumerate(kf.split(image_paths)):\n",
    "#     print(f\"Starting Fold {fold+1}/{n_splits}\")\n",
    "    \n",
    "#     # Split into training and validation\n",
    "#     train_images = [image_paths[i] for i in train_idx]\n",
    "#     train_labels = labels[train_idx]\n",
    "#     val_images = [image_paths[i] for i in val_idx]\n",
    "#     val_labels = labels[val_idx]\n",
    "# for fold, (train_idx, val_idx) in enumerate(kf.split(image_paths)):\n",
    "#     print(f\"\\nStarting Fold {fold + 1}/5\")\n",
    "    \n",
    "#     # Train and validation splits\n",
    "#     train_images = image_paths[train_idx]\n",
    "#     train_labels = labels[train_idx]\n",
    "#     val_images = image_paths[val_idx]\n",
    "#     val_labels = labels[val_idx]\n",
    "\n",
    "# Convert image_paths and labels to NumPy arrays\n",
    "image_paths = np.array(image_paths)  # Convert the list of file paths to a NumPy array\n",
    "labels = np.array(labels)            # Convert the list of labels to a NumPy array\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(image_paths)):\n",
    "    print(f\"\\nStarting Fold {fold + 1}/5\")\n",
    "    \n",
    "    # Convert indices to NumPy arrays (just in case)\n",
    "    train_idx = np.array(train_idx)\n",
    "    val_idx = np.array(val_idx)\n",
    "\n",
    "    # Split data using the indices\n",
    "    train_images = image_paths[train_idx]  # Train image paths\n",
    "    train_labels = labels[train_idx]       # Train labels\n",
    "    val_images = image_paths[val_idx]      # Validation image paths\n",
    "    val_labels = labels[val_idx]           # Validation labels\n",
    "\n",
    "    print(f\"Fold {fold + 1}: Training on {len(train_images)} samples, Validating on {len(val_images)} samples\")\n",
    "print(f\"Type of train_idx: {type(train_idx)}, Shape: {train_idx.shape}\")\n",
    "print(f\"Type of train_images: {type(train_images)}, Shape: {len(train_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0758c6-add8-471f-9cb5-40458b04d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators\n",
    "train_generator = dual_input_generator(train_images, train_labels, batch_size, input_shape)\n",
    "val_generator = dual_input_generator(val_images, val_labels, batch_size, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758232e2-c0d0-45d0-9857-d9ba05593ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the model\n",
    "model = build_dual_stream_model(input_shape, num_classes)\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1ade6c-0004-445f-b5e9-e296ad088411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "    # Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_images) // batch_size,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(val_images) // batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[lr_scheduler, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2382c487-3b6c-4907-88be-9ee56f871f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "val_loss, val_accuracy = model.evaluate(val_generator, steps=len(val_images) // batch_size)\n",
    "print(f\"Fold {fold+1} - Validation Accuracy: {val_accuracy:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Store results\n",
    "fold_accuracies.append(val_accuracy)\n",
    "fold_losses.append(val_loss)\n",
    "\n",
    "# Final Cross-Validation Results\n",
    "print(\"\\nFinal Cross-Validation Results:\")\n",
    "print(f\"Average Accuracy: {np.mean(fold_accuracies):.4f}, Average Loss: {np.mean(fold_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4487dea5-35de-40cc-8f94-0f6b239ce666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
