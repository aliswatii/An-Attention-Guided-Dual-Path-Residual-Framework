{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5bc620e-312f-49b8-bef3-80d48ea39cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Multiply, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ea937f2-ec0b-4022-82cd-0579ea4af886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9740 images from 4 classes:\n",
      "Class 0: MildDemented\n",
      "Class 1: ModerateDemented\n",
      "Class 2: NonDemented\n",
      "Class 3: VeryMildDemented\n"
     ]
    }
   ],
   "source": [
    "# Dataset directory\n",
    "dataset_dir = r\"dir\"  # Replace with your dataset path\n",
    "\n",
    "# Function to load image paths and corresponding labels\n",
    "def load_image_paths_and_labels(dataset_dir):\n",
    "    image_paths = []  # To store all image file paths\n",
    "    labels = []       # To store corresponding labels as integers\n",
    "    class_names = sorted(os.listdir(dataset_dir))  # Sort class names alphabetically for consistent labeling\n",
    "\n",
    "    for label, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(dataset_dir, class_name)\n",
    "        if os.path.isdir(class_dir):  # Ensure it's a directory\n",
    "            for file_name in os.listdir(class_dir):\n",
    "                if file_name.lower().endswith(('.png', '.jpg', '.jpeg')):  # Check for valid image extensions\n",
    "                    file_path = os.path.join(class_dir, file_name)\n",
    "                    image_paths.append(file_path)\n",
    "                    labels.append(label)  # Assign label based on the class index\n",
    "\n",
    "    return image_paths, labels, class_names\n",
    "\n",
    "# Load data\n",
    "image_paths, labels, class_names = load_image_paths_and_labels(dataset_dir)\n",
    "\n",
    "# Convert labels to numpy array\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"Loaded {len(image_paths)} images from {len(class_names)} classes:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"Class {i}: {class_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc550a68-31c5-4b48-bb05-7b3a6899609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation settings\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d279ebe1-f965-4fd5-8e65-4c1c59ddbdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom generator to yield dual inputs\n",
    "def dual_input_generator(image_paths, labels, batch_size, input_shape):\n",
    "    while True:\n",
    "        indices = np.random.choice(len(image_paths), batch_size)\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "\n",
    "        for i in indices:\n",
    "            img = tf.keras.utils.load_img(image_paths[i], target_size=input_shape[:2])\n",
    "            img = tf.keras.utils.img_to_array(img) / 255.0  # Normalize to [0, 1]\n",
    "            batch_images.append(img)\n",
    "            batch_labels.append(labels[i])\n",
    "\n",
    "        batch_images = np.array(batch_images)\n",
    "        batch_labels = tf.keras.utils.to_categorical(batch_labels, num_classes=4)  # Convert labels to one-hot\n",
    "\n",
    "        # Yield two identical inputs for dual-stream model\n",
    "        yield (batch_images, batch_images), batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7553884b-5979-4894-9c89-cb85ecb183c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a CNN stream by creating a custom layer with ResNet50 features\n",
    "class CNNStream(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_shape, name_suffix):\n",
    "        super(CNNStream, self).__init__(name=f\"CNNStream_{name_suffix}\")\n",
    "        self.base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape, name=f'resnet_{name_suffix}')\n",
    "        self.pooling = GlobalAveragePooling2D()\n",
    "        self.fc1 = Dense(128, activation='relu')\n",
    "        self.dropout = Dropout(0.4)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.base_model(inputs)\n",
    "        x = self.pooling(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4ede7c5-2e3f-4799-b557-48bac518aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the soft attention mechanism\n",
    "def soft_attention(fusion_features):\n",
    "    attention = Dense(128, activation='sigmoid')(fusion_features)\n",
    "    attention = Dense(1, activation='sigmoid')(attention)\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdb112d8-33d0-4cae-aed3-e4bba5e2769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Dual-Stream Model with Soft Attention\n",
    "def build_dual_stream_model(input_shape, num_classes):\n",
    "    # Stream 1 with unique name suffix\n",
    "    input1 = Input(shape=input_shape)\n",
    "    stream1_features = CNNStream(input_shape, 'stream1')(input1)\n",
    "    \n",
    "    # Stream 2 with a different name suffix\n",
    "    input2 = Input(shape=input_shape)\n",
    "    stream2_features = CNNStream(input_shape, 'stream2')(input2)\n",
    "\n",
    "    # Concatenate the outputs of both streams\n",
    "    fusion_features = Concatenate()([stream1_features, stream2_features])\n",
    "\n",
    "    # Apply the soft attention mechanism\n",
    "    attention = soft_attention(fusion_features)\n",
    "    \n",
    "    # Multiply attention weights with the fusion features\n",
    "    attended_features = Multiply()([fusion_features, attention])\n",
    "\n",
    "    # Fully connected layer for classification\n",
    "    x = Dense(256, activation='relu')(attended_features)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = Model(inputs=[input1, input2], outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71612fd4-6e88-4029-a66e-f12c760f68ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Fold 1/5\n",
      "Fold 1: Training on 7792 samples, Validating on 1948 samples\n",
      "\n",
      "Starting Fold 2/5\n",
      "Fold 2: Training on 7792 samples, Validating on 1948 samples\n",
      "\n",
      "Starting Fold 3/5\n",
      "Fold 3: Training on 7792 samples, Validating on 1948 samples\n",
      "\n",
      "Starting Fold 4/5\n",
      "Fold 4: Training on 7792 samples, Validating on 1948 samples\n",
      "\n",
      "Starting Fold 5/5\n",
      "Fold 5: Training on 7792 samples, Validating on 1948 samples\n",
      "Type of train_idx: <class 'numpy.ndarray'>, Shape: (7792,)\n",
      "Type of train_images: <class 'numpy.ndarray'>, Shape: 7792\n"
     ]
    }
   ],
   "source": [
    "# Initialize KFold\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Parameters\n",
    "input_shape = (174, 208, 3)  # Adjust based on your image size\n",
    "num_classes = 4  # 4 classes\n",
    "batch_size = 32\n",
    "epochs = 40\n",
    "\n",
    "# Store results\n",
    "fold_accuracies = []\n",
    "fold_losses = []\n",
    "\n",
    "# for fold, (train_idx, val_idx) in enumerate(kf.split(image_paths)):\n",
    "#     print(f\"Starting Fold {fold+1}/{n_splits}\")\n",
    "    \n",
    "#     # Split into training and validation\n",
    "#     train_images = [image_paths[i] for i in train_idx]\n",
    "#     train_labels = labels[train_idx]\n",
    "#     val_images = [image_paths[i] for i in val_idx]\n",
    "#     val_labels = labels[val_idx]\n",
    "# for fold, (train_idx, val_idx) in enumerate(kf.split(image_paths)):\n",
    "#     print(f\"\\nStarting Fold {fold + 1}/5\")\n",
    "    \n",
    "#     # Train and validation splits\n",
    "#     train_images = image_paths[train_idx]\n",
    "#     train_labels = labels[train_idx]\n",
    "#     val_images = image_paths[val_idx]\n",
    "#     val_labels = labels[val_idx]\n",
    "\n",
    "# Convert image_paths and labels to NumPy arrays\n",
    "image_paths = np.array(image_paths)  # Convert the list of file paths to a NumPy array\n",
    "labels = np.array(labels)            # Convert the list of labels to a NumPy array\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(image_paths)):\n",
    "    print(f\"\\nStarting Fold {fold + 1}/5\")\n",
    "    \n",
    "    # Convert indices to NumPy arrays (just in case)\n",
    "    train_idx = np.array(train_idx)\n",
    "    val_idx = np.array(val_idx)\n",
    "\n",
    "    # Split data using the indices\n",
    "    train_images = image_paths[train_idx]  # Train image paths\n",
    "    train_labels = labels[train_idx]       # Train labels\n",
    "    val_images = image_paths[val_idx]      # Validation image paths\n",
    "    val_labels = labels[val_idx]           # Validation labels\n",
    "\n",
    "    print(f\"Fold {fold + 1}: Training on {len(train_images)} samples, Validating on {len(val_images)} samples\")\n",
    "print(f\"Type of train_idx: {type(train_idx)}, Shape: {train_idx.shape}\")\n",
    "print(f\"Type of train_images: {type(train_images)}, Shape: {len(train_images)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "128bcd12-7a0f-4712-acfa-e76db9c6bf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators\n",
    "train_generator = dual_input_generator(train_images, train_labels, batch_size, input_shape)\n",
    "val_generator = dual_input_generator(val_images, val_labels, batch_size, input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1049ddb7-2e4d-4c8c-8cb2-87667343c736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\haider\\anaconda3\\envs\\ali\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build and compile the model\n",
    "model = build_dual_stream_model(input_shape, num_classes)\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "183e5146-1872-4d48-8f48-c4107ce5f841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3329s\u001b[0m 13s/step - accuracy: 0.6269 - loss: 0.8754 - val_accuracy: 0.2797 - val_loss: 1.5693 - learning_rate: 1.0000e-04\n",
      "Epoch 2/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3195s\u001b[0m 13s/step - accuracy: 0.8705 - loss: 0.3609 - val_accuracy: 0.3167 - val_loss: 1.5979 - learning_rate: 1.0000e-04\n",
      "Epoch 3/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3149s\u001b[0m 13s/step - accuracy: 0.9231 - loss: 0.2270 - val_accuracy: 0.4323 - val_loss: 1.3153 - learning_rate: 1.0000e-04\n",
      "Epoch 4/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3152s\u001b[0m 13s/step - accuracy: 0.9419 - loss: 0.1676 - val_accuracy: 0.6849 - val_loss: 1.2475 - learning_rate: 1.0000e-04\n",
      "Epoch 5/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3157s\u001b[0m 13s/step - accuracy: 0.9526 - loss: 0.1259 - val_accuracy: 0.7474 - val_loss: 0.8972 - learning_rate: 1.0000e-04\n",
      "Epoch 6/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3155s\u001b[0m 13s/step - accuracy: 0.9644 - loss: 0.0922 - val_accuracy: 0.7807 - val_loss: 0.7210 - learning_rate: 1.0000e-04\n",
      "Epoch 7/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3157s\u001b[0m 13s/step - accuracy: 0.9698 - loss: 0.0823 - val_accuracy: 0.7807 - val_loss: 0.6539 - learning_rate: 1.0000e-04\n",
      "Epoch 8/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3155s\u001b[0m 13s/step - accuracy: 0.9721 - loss: 0.0749 - val_accuracy: 0.9323 - val_loss: 0.1805 - learning_rate: 1.0000e-04\n",
      "Epoch 9/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3153s\u001b[0m 13s/step - accuracy: 0.9692 - loss: 0.0818 - val_accuracy: 0.9187 - val_loss: 0.2016 - learning_rate: 1.0000e-04\n",
      "Epoch 10/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3163s\u001b[0m 13s/step - accuracy: 0.9685 - loss: 0.0745 - val_accuracy: 0.8760 - val_loss: 0.5035 - learning_rate: 1.0000e-04\n",
      "Epoch 11/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3162s\u001b[0m 13s/step - accuracy: 0.9669 - loss: 0.0750 - val_accuracy: 0.9380 - val_loss: 0.1800 - learning_rate: 1.0000e-04\n",
      "Epoch 12/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3154s\u001b[0m 13s/step - accuracy: 0.9773 - loss: 0.0572 - val_accuracy: 0.9286 - val_loss: 0.1803 - learning_rate: 1.0000e-04\n",
      "Epoch 13/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3153s\u001b[0m 13s/step - accuracy: 0.9821 - loss: 0.0410 - val_accuracy: 0.8990 - val_loss: 0.3660 - learning_rate: 1.0000e-04\n",
      "Epoch 14/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3153s\u001b[0m 13s/step - accuracy: 0.9764 - loss: 0.0614 - val_accuracy: 0.9318 - val_loss: 0.2110 - learning_rate: 1.0000e-04\n",
      "Epoch 15/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3159s\u001b[0m 13s/step - accuracy: 0.9754 - loss: 0.0573 - val_accuracy: 0.8911 - val_loss: 0.4063 - learning_rate: 1.0000e-04\n",
      "Epoch 16/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - accuracy: 0.9762 - loss: 0.0554 \n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3158s\u001b[0m 13s/step - accuracy: 0.9762 - loss: 0.0554 - val_accuracy: 0.8521 - val_loss: 0.4087 - learning_rate: 1.0000e-04\n",
      "Epoch 17/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3156s\u001b[0m 13s/step - accuracy: 0.9800 - loss: 0.0469 - val_accuracy: 0.9563 - val_loss: 0.1272 - learning_rate: 5.0000e-05\n",
      "Epoch 18/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3160s\u001b[0m 13s/step - accuracy: 0.9817 - loss: 0.0347 - val_accuracy: 0.9552 - val_loss: 0.1538 - learning_rate: 5.0000e-05\n",
      "Epoch 19/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3154s\u001b[0m 13s/step - accuracy: 0.9860 - loss: 0.0295 - val_accuracy: 0.9505 - val_loss: 0.1664 - learning_rate: 5.0000e-05\n",
      "Epoch 20/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3157s\u001b[0m 13s/step - accuracy: 0.9852 - loss: 0.0241 - val_accuracy: 0.9417 - val_loss: 0.2177 - learning_rate: 5.0000e-05\n",
      "Epoch 21/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3157s\u001b[0m 13s/step - accuracy: 0.9802 - loss: 0.0421 - val_accuracy: 0.9594 - val_loss: 0.1314 - learning_rate: 5.0000e-05\n",
      "Epoch 22/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - accuracy: 0.9833 - loss: 0.0303 \n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3156s\u001b[0m 13s/step - accuracy: 0.9833 - loss: 0.0303 - val_accuracy: 0.9594 - val_loss: 0.1589 - learning_rate: 5.0000e-05\n",
      "Epoch 23/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3184s\u001b[0m 13s/step - accuracy: 0.9871 - loss: 0.0246 - val_accuracy: 0.9630 - val_loss: 0.1351 - learning_rate: 2.5000e-05\n",
      "Epoch 24/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3315s\u001b[0m 14s/step - accuracy: 0.9887 - loss: 0.0193 - val_accuracy: 0.9516 - val_loss: 0.2116 - learning_rate: 2.5000e-05\n",
      "Epoch 25/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3320s\u001b[0m 14s/step - accuracy: 0.9859 - loss: 0.0221 - val_accuracy: 0.9495 - val_loss: 0.1818 - learning_rate: 2.5000e-05\n",
      "Epoch 26/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3313s\u001b[0m 14s/step - accuracy: 0.9856 - loss: 0.0252 - val_accuracy: 0.9594 - val_loss: 0.1481 - learning_rate: 2.5000e-05\n",
      "Epoch 27/40\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13s/step - accuracy: 0.9894 - loss: 0.0210 \n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3332s\u001b[0m 14s/step - accuracy: 0.9894 - loss: 0.0210 - val_accuracy: 0.9641 - val_loss: 0.1364 - learning_rate: 2.5000e-05\n",
      "Epoch 27: early stopping\n",
      "Restoring model weights from the end of the best epoch: 17.\n"
     ]
    }
   ],
   "source": [
    "# Callbacks\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "    # Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_images) // batch_size,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(val_images) // batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[lr_scheduler, early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0710a95-6264-4de1-8c56-51df36e1bbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 3s/step - accuracy: 0.9427 - loss: 0.1566\n",
      "Fold 5 - Validation Accuracy: 0.9484, Validation Loss: 0.1507\n",
      "\n",
      "Final Cross-Validation Results:\n",
      "Average Accuracy: 0.9484, Average Loss: 0.1507\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "val_loss, val_accuracy = model.evaluate(val_generator, steps=len(val_images) // batch_size)\n",
    "print(f\"Fold {fold+1} - Validation Accuracy: {val_accuracy:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Store results\n",
    "fold_accuracies.append(val_accuracy)\n",
    "fold_losses.append(val_loss)\n",
    "\n",
    "# Final Cross-Validation Results\n",
    "print(\"\\nFinal Cross-Validation Results:\")\n",
    "print(f\"Average Accuracy: {np.mean(fold_accuracies):.4f}, Average Loss: {np.mean(fold_losses):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3114b42d-2857-4b4f-941d-8073b832b4c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce677b4-4fdb-443f-9a51-ec0d80956f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
